{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad0db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actor\n",
    "\n",
    "function 1, make decision: \n",
    "\n",
    "    input (s_t), output(action_t)\n",
    "\n",
    "function 2, update decisition model:\n",
    "\n",
    "    input (learning_rate, G_t), output(new model)\n",
    "    \n",
    "function 3, update the tagrget actor network\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorlayer.layers import (BatchNorm, Conv2d, Dropout , Dense, Flatten, Input, LocalResponseNorm, MaxPool2d)\n",
    "from tensorlayer.models import Model\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91076272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "#   Actor\n",
    "# ===========================\n",
    "\n",
    "\n",
    "class Actor_NetWork(object):\n",
    "    '''\n",
    "    tf_session: tensorflow session\n",
    "    state_shape: shape of state\n",
    "    action_shape: \n",
    "    learning_rate: learning rate of actor\n",
    "    target_lr : learning rate of target actor network\n",
    "    batch size: size of mini batch, used to training\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, gamma, state_shape, action_shape, a_learning_rate):\n",
    "        \n",
    "        self.gamma = gamma # discount_factor \n",
    "        \n",
    "        self.state_shape = state_shape # should be [None, m_stock, historic_window, feature]\n",
    "        self.action_shape = action_shape # should be [None, m_stock]\n",
    "        \n",
    "        # Acotr Network\n",
    "        self.actor_learning_rate = a_learning_rate\n",
    "        self.actor_network = self.get_cnn_actor_model(self.state_shape ,\"Actor_Network\") # tensorlayer model\n",
    "        self.actor_network.train()\n",
    "        self.actor_opt = tf.optimizers.Adam(self.actor_learning_rate)\n",
    "  \n",
    "\n",
    "        \n",
    "  \n",
    "    \n",
    "    def Generate_action(self, states, greedy = False):\n",
    "        '''\n",
    "        states shape should be [m_stock, historic_window, feature]\n",
    "        greedy is used to determine random explore \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if greedy:\n",
    "            new_action = self.actor_network(states)\n",
    "            return new_action\n",
    "        else:\n",
    "            new_action = self.actor_network(states) \n",
    "            new_action = new_action + np.random.normal(0, 0.01, np.shape(new_action))\n",
    "            new_action = np.clip(new_action, 0 ,1) # values outside the interval are clipped to the interval edges\n",
    "            new_action = new_action/np.sum(new_action)\n",
    "            new_action = np.array(new_action).astype(np.float32)\n",
    "            return new_action \n",
    "    \n",
    "    def learn(self, inputs, t, Gt):\n",
    "        '''\n",
    "        inputs: (states_t,actions_t,rewards_t,states_t+1)\n",
    "        inputs shape: [[batch_size, m_stock, historic_window, feature], [batch_size, actions] \\\n",
    "            ,[batch_size], [[batch_size, m_stock, historic_window, feature]]\n",
    "            \n",
    "        used to update network\n",
    "        \n",
    "        '''\n",
    "        states = inputs[0]\n",
    "        actions =  inputs[1]\n",
    "        rewards = inputs[2]\n",
    "        next_states = inputs[3]\n",
    "        \n",
    "        discount_factor = tf.constant(self.gamma ** t, dtype=tf.float32)\n",
    "        G_t = tf.constant(Gt, dtype=tf.float32)\n",
    "        \n",
    "        # actor gradients - Monte Carlo Policy Gradient\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_network(states)\n",
    "        actor_grads = discount_factor * G_t * tape.gradient(actions, self.actor_network.trainable_weights)\n",
    "        \n",
    "        # update actor and critic\n",
    "        self.actor_opt.apply_gradients(zip(actor_grads, self.actor_network.trainable_weights))\n",
    "        \n",
    "\n",
    "           \n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([\"MCPG\", \"PM\"]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path) # create a new dir\n",
    "        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor_network)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        path = os.path.join('model', '_'.join([\"MCPG\", \"PM\"]))\n",
    "        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor_network)\n",
    "    \n",
    "\n",
    "    def get_cnn_actor_model(self, inputs_shape, model_name):\n",
    "        # self defined initialization\n",
    "        stock_num = inputs_shape[1]\n",
    "        his_window = inputs_shape[2]\n",
    "        feature_num = inputs_shape[3]\n",
    "        W_init = tl.initializers.truncated_normal(stddev=5e-2)\n",
    "        W_init2 = tl.initializers.truncated_normal(stddev=0.04)\n",
    "        b_init2 = tl.initializers.constant(value=0.1)\n",
    "\n",
    "        # build network\n",
    "        ni = Input(inputs_shape)\n",
    "        nn = Conv2d(feature_num, (1, 1), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv1')(ni) #fully connected\n",
    "        nn = MaxPool2d((3, 3), (2, 2), padding='SAME', name='pool1')(nn)\n",
    "\n",
    "        nn = Conv2d(feature_num, (1, his_window), (1, 1), padding='SAME', act=tf.nn.relu, W_init=W_init, b_init=None, name='conv2')(nn)\n",
    "        nn = MaxPool2d((3, 3), (2, 2), padding='SAME', name='pool2')(nn)\n",
    "\n",
    "        nn = Flatten(name='flatten')(nn)\n",
    "        nn = Dropout(keep=0.5)(nn)\n",
    "        nn = Dense(32, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='dense1relu')(nn)\n",
    "        nn = BatchNorm()(nn)\n",
    "        nn = Dense(32, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='dense2relu')(nn)\n",
    "        nn = BatchNorm()(nn)\n",
    "        nn = Dense(stock_num, act=tf.nn.softmax, W_init=W_init2, name='output')(nn)\n",
    "        M = Model(inputs=ni, outputs=nn, name=model_name)\n",
    "        return M\n",
    "           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe04d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# import our model and env\n",
    "from env import PortfolioEnv\n",
    "from ddpg import actor_critic ,replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fix_yahoo_finance as yf\n",
    "\n",
    "stocks =  ['amt', 'axp', 'ba', 'cvx', 'jnj', 'ko', 'mcd', 'msft', 't', 'wmt']\n",
    "start = datetime(2012,1,1)\n",
    "end = datetime(2022,11,11)\n",
    "data = yf.download(stocks, start=start, end=end)\n",
    "data_close = data['Adj Close']\n",
    "data_close.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using US stock\n",
    "\n",
    "\n",
    "df = data.drop('Close', axis=1)\n",
    "df = df[['Open','High','Low','Adj Close','Volume']]\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "df.sort_index(axis=1, level=0, inplace=True)\n",
    "df.columns = pd.MultiIndex.from_product([stocks,['open', 'high', 'low', 'close', \"volume\"]], \n",
    "                                        names=['stock', 'price'])\n",
    "df = df.dropna()\n",
    "\n",
    "start = datetime.strptime(\"2013-01-01\", '%Y-%m-%d' )\n",
    "df_ = df.loc[start:]\n",
    "\n",
    "# set the env input\n",
    "history = df_ # the stock data\n",
    "abbreviation = stocks # name of stock\n",
    "steps = 100 # step for one eps\n",
    "trading_cost = 0.0025\n",
    "time_cost=0.00 # cost of holding equity\n",
    "window_length = 10 # for obs\n",
    "eps_move = 0 # move the start date after each rest\n",
    "sample_start_date = \"2018-05-01\"\n",
    "\n",
    "# build the replay_buffer\n",
    "buffer_size = 50 # the max size of buffer\n",
    "\n",
    "# build the actor, critic, and target network \n",
    "gamma = 0.99986 # discount factor\n",
    "state_shape = [None,len(abbreviation),window_length,5] # (none, m_stock, history_window, feature)\n",
    "action_shape = [None,len(abbreviation)] # (none, m_stock)\n",
    "a_learning_rate = 0.001 # learning rate for actor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0935f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out agents\n",
    "#  gamma, state_shape, action_shape, a_learning_rate, target_lr, c_learning_rate\n",
    "a = actor_critic.Actor_Critic_NetWork(gamma, state_shape,action_shape,a_learning_rate,target_lr,c_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1234\n",
    "\n",
    "# build the env\n",
    "PM_env = PortfolioEnv.PortfolioEnv(history, abbreviation, steps, trading_cost, time_cost, window_length, eps_move, sample_start_date)\n",
    "\n",
    "# build the replay buffer\n",
    "rb = replay_buffer.ReplayBuffer(buffer_size, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8013384",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "state_dim = PM_env.observation_space.shape\n",
    "action_dim = PM_env.action_space.shape\n",
    "action_range = PM_env.action_space.high  # scale action, [-action_range, action_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "TRAIN_EPISODES = 5\n",
    "\n",
    "batch_size = 50 # mini batch\n",
    "\n",
    "train = True\n",
    "RENDER = False\n",
    "\n",
    "ALG_NAME = \"DDPG\"\n",
    "ENV_ID = \"Portfolio_Env\"\n",
    "\n",
    "t0 = time.time()\n",
    "if train:  # train\n",
    "    all_episode_reward = []\n",
    "    for episode in range(TRAIN_EPISODES):\n",
    "        state, info = PM_env.reset() \n",
    "        episode_reward = 0\n",
    "        for step in range(steps):\n",
    "            #print(state)\n",
    "            if RENDER:\n",
    "                PM_env.render()\n",
    "            # Add exploration noise\n",
    "            action = ac.Generate_action(state, greedy = False) #[[a0, a1, a2]]\n",
    "            #print(action)\n",
    "            state_, reward, done, info = PM_env.step(action) # compute the immidate reward and move to next steps\n",
    "            #print(info)\n",
    "            rb.add(state, action, reward, state_) # replay buffer add memory\n",
    "\n",
    "            if rb.count > buffer_size:\n",
    "                inputs = rb.sample_batch(batch_size)\n",
    "                ac.learn(inputs)\n",
    "\n",
    "            state = state_\n",
    "            episode_reward += reward * gamma**step\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode == 0:\n",
    "            all_episode_reward.append(episode_reward)\n",
    "        else:\n",
    "            all_episode_reward.append(all_episode_reward[-1] * 0.9 + episode_reward * 0.1)\n",
    "        print(\n",
    "            'Training  | Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                episode + 1, TRAIN_EPISODES, episode_reward,\n",
    "                time.time() - t0\n",
    "            )\n",
    "        )\n",
    "    ac.save()\n",
    "    plt.plot(all_episode_reward)\n",
    "    if not os.path.exists('image'):\n",
    "        os.makedirs('image')\n",
    "    plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933532b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e66d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
